{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4gq6MnxKL_S"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make neccessary imports and installations (I am using A100 GPU runtime)\n",
        "import json\n",
        "import subprocess\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel"
      ],
      "metadata": {
        "id": "N8ntm4S9KSYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Give a hugging face access token to access mistral gated model\n",
        "login(\"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\")"
      ],
      "metadata": {
        "id": "-acD3SN3KSay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import Dataset\n",
        "def load_jsonl_to_dataset(path: str, logs_per_sample: int = 10) -> Dataset:\n",
        "    with open(path, \"r\") as f:\n",
        "        lines = [json.loads(line) for line in f]\n",
        "    examples = []\n",
        "    for i in range(0, len(lines), logs_per_sample):\n",
        "        chunk = lines[i : i + logs_per_sample]\n",
        "        inp = \"\\n\".join(json.dumps(evt, indent=2) for evt in chunk)\n",
        "        errs = [evt for evt in chunk if evt.get(\"log_level\") == \"ERROR\"]\n",
        "        traces = [\n",
        "            f\"- Trace ID: {evt.get('trace_id','?')} in {evt.get('service','?')} — Exception: {evt.get('exception','None')}\"\n",
        "            for evt in errs\n",
        "        ]\n",
        "        out = (\n",
        "            f\"There are {len(errs)} ERROR log(s).\\n\\n\"\n",
        "            f\"The order flow includes: {' → '.join(dict.fromkeys([evt['service'] for evt in chunk]))}.\\n\\n\"\n",
        "            \"Error(s) occurred at the following trace IDs:\\n\"\n",
        "            + (\"\\n\".join(traces) if traces else \"None\")\n",
        "        )\n",
        "        examples.append({\n",
        "            \"instruction\": \"Analyze the following application logs to identify number of errors, order flow, and trace IDs for failures.\",\n",
        "            \"input\": inp,\n",
        "            \"output\": out,\n",
        "        })\n",
        "    return Dataset.from_list(examples)\n",
        "\n",
        "dataset = load_jsonl_to_dataset(\"order_logs.jsonl\")\n"
      ],
      "metadata": {
        "id": "xLQIuqtGKSc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "def tokenize_and_mask(example, tokenizer, max_seq=1024, max_prompt=512):\n",
        "    prompt = (\n",
        "        \"### Instruction:\\n\" + example[\"instruction\"] +\n",
        "        \"\\n\\n### Input:\\n\"    + example[\"input\"] +\n",
        "        \"\\n\\n### Response:\\n\"\n",
        "    )\n",
        "    response = example[\"output\"] + tokenizer.eos_token\n",
        "\n",
        "    # Tokenize prompt and response separately\n",
        "    pid = tokenizer(prompt, add_special_tokens=False).input_ids\n",
        "    rid = tokenizer(response, add_special_tokens=False).input_ids\n",
        "\n",
        "    # Truncate prompt if too long\n",
        "    if len(pid) > max_prompt:\n",
        "        pid = pid[-max_prompt:]\n",
        "    # Concatenate and truncate to max_seq\n",
        "    input_ids = (pid + rid)[:max_seq]\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Build labels: mask the prompt, keep response\n",
        "    labels = [-100] * len(pid) + rid\n",
        "    labels = labels[: len(input_ids)]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "tokenized = dataset.map(\n",
        "    partial(tokenize_and_mask, tokenizer=tokenizer),\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "# ===============================================\n",
        "# 5) Load Mistral-7B in FP16 (no meta tensors), move to GPU\n",
        "# ===============================================\n",
        "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=False,\n",
        "    device_map=None\n",
        ")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_cpu = model_cpu.to(device)\n"
      ],
      "metadata": {
        "id": "pzaK4pWXKSe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model_cpu, lora_config)"
      ],
      "metadata": {
        "id": "FQJAZLrvKShI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./log-analyzer-adapter\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"./log-analyzer_\")\n",
        "tokenizer.save_pretrained(\"./log-analyzer_\")\n"
      ],
      "metadata": {
        "id": "oLqlceSGKSjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_for_merge = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=False,\n",
        "    device_map=None\n",
        ").to(device)\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(base_for_merge, \"./log-analyzer-adapter\")\n",
        "merged = peft_model.merge_and_unload()\n",
        "merged.save_pretrained(\"./log-analyzer_\")\n",
        "tokenizer.save_pretrained(\"./log-analyzer_\")"
      ],
      "metadata": {
        "id": "_5j_KEZfKSlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECKING OUTPUT before proceedinb to command prompt\n",
        "\n",
        "\n",
        "# 1) Install bitsandbytes if you haven’t already (for 4-bit inference)\n",
        "!pip install bitsandbytes\n",
        "\n",
        "# 2) Imports\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# 3) Settings\n",
        "MODEL_DIR = \"./log-analyzer-merged\"\n",
        "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 4) Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# 5) Load the merged model in 4-bit\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    quantization_config=bnb_cfg,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()  # turn off dropout, etc.\n",
        "\n",
        "# 6) Inference helper\n",
        "def analyze_logs_chunk(log_chunk: str, max_new_tokens: int = 200) -> str:\n",
        "    prompt = (\n",
        "        \"### Instruction:\\n\"\n",
        "        \"Analyze the following application logs to identify number of errors, order flow, and trace IDs for failures.\\n\\n\"\n",
        "        \"### Input:\\n\"\n",
        "        f\"{log_chunk}\\n\\n\"\n",
        "        \"### Response:\\n\"\n",
        "    )\n",
        "    # tokenize + move to GPU\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(DEVICE)\n",
        "    # generate only new tokens\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.1,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    # slice off prompt, decode only new tokens\n",
        "    gen = out[0][ inputs[\"input_ids\"].size(1) : ]\n",
        "    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "\n",
        "# 7) Load a small test chunk\n",
        "with open(\"order_logs.jsonl\") as f:\n",
        "    logs = [json.loads(l) for l in f]\n",
        "test_chunk = \"\\n\".join(json.dumps(evt, indent=2) for evt in logs[:5])\n",
        "\n",
        "# 8) Run and print\n",
        "print(\"=== Sanity Check Output ===\\n\")\n",
        "print(analyze_logs_chunk(test_chunk))\n"
      ],
      "metadata": {
        "id": "kXkkojHxKSnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert this\n",
        "%cd /content\n",
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO_yN0dQKSpP",
        "outputId": "21272ab6-f374-4af4-e5cc-732fbc780929"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'llama.cpp'...\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp\n"
      ],
      "metadata": {
        "id": "tVSpCWU3KSrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install the requirements\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "pUoSFFM7KStj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert final model to gguf format to deploy on ollama, openwebUI\n",
        "!python3 convert_hf_to_gguf.py /content/log-analyzer_ --outfile /content/log-analyzer_.gguf\n"
      ],
      "metadata": {
        "id": "5Jm9Ave3KSvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save this gguf to your local environment to download quicker(optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p /content/drive/MyDrive/llm_models\n",
        "!cp /content/log-analyzer-merged_v6.gguf /content/drive/MyDrive/llm_models/"
      ],
      "metadata": {
        "id": "3o8eEhbSKSxg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}