# -*- coding: utf-8 -*-
"""LLM_finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I-I6tYuJiFfReUpV-CzuWdyL3BF9Nrye
"""

!nvidia-smi

# make neccessary imports and installations (I am using A100 GPU runtime)
import json
import subprocess
import torch
from huggingface_hub import login
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
)
from peft import LoraConfig, get_peft_model, PeftModel

# Give a hugging face access token to access mistral gated model
login("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")

!pip install datasets
from datasets import Dataset
def load_jsonl_to_dataset(path: str, logs_per_sample: int = 10) -> Dataset:
    with open(path, "r") as f:
        lines = [json.loads(line) for line in f]
    examples = []
    for i in range(0, len(lines), logs_per_sample):
        chunk = lines[i : i + logs_per_sample]
        inp = "\n".join(json.dumps(evt, indent=2) for evt in chunk)
        errs = [evt for evt in chunk if evt.get("log_level") == "ERROR"]
        traces = [
            f"- Trace ID: {evt.get('trace_id','?')} in {evt.get('service','?')} — Exception: {evt.get('exception','None')}"
            for evt in errs
        ]
        out = (
            f"There are {len(errs)} ERROR log(s).\n\n"
            f"The order flow includes: {' → '.join(dict.fromkeys([evt['service'] for evt in chunk]))}.\n\n"
            "Error(s) occurred at the following trace IDs:\n"
            + ("\n".join(traces) if traces else "None")
        )
        examples.append({
            "instruction": "Analyze the following application logs to identify number of errors, order flow, and trace IDs for failures.",
            "input": inp,
            "output": out,
        })
    return Dataset.from_list(examples)

dataset = load_jsonl_to_dataset("order_logs.jsonl")

from functools import partial

def tokenize_and_mask(example, tokenizer, max_seq=1024, max_prompt=512):
    prompt = (
        "### Instruction:\n" + example["instruction"] +
        "\n\n### Input:\n"    + example["input"] +
        "\n\n### Response:\n"
    )
    response = example["output"] + tokenizer.eos_token

    # Tokenize prompt and response separately
    pid = tokenizer(prompt, add_special_tokens=False).input_ids
    rid = tokenizer(response, add_special_tokens=False).input_ids

    # Truncate prompt if too long
    if len(pid) > max_prompt:
        pid = pid[-max_prompt:]
    # Concatenate and truncate to max_seq
    input_ids = (pid + rid)[:max_seq]
    attention_mask = [1] * len(input_ids)

    # Build labels: mask the prompt, keep response
    labels = [-100] * len(pid) + rid
    labels = labels[: len(input_ids)]

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels,
    }

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

tokenized = dataset.map(
    partial(tokenize_and_mask, tokenizer=tokenizer),
    remove_columns=dataset.column_names
)

# ===============================================
# 5) Load Mistral-7B in FP16 (no meta tensors), move to GPU
# ===============================================
model_cpu = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    torch_dtype=torch.float16,
    low_cpu_mem_usage=False,
    device_map=None
)
device = "cuda" if torch.cuda.is_available() else "cpu"
model_cpu = model_cpu.to(device)

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model_cpu, lora_config)

training_args = TrainingArguments(
    output_dir="./log-analyzer-adapter",
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=2,
    learning_rate=2e-4,
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized,
    tokenizer=tokenizer,
)

trainer.train()
trainer.save_model("./log-analyzer_")
tokenizer.save_pretrained("./log-analyzer_")

base_for_merge = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    torch_dtype=torch.float16,
    low_cpu_mem_usage=False,
    device_map=None
).to(device)

peft_model = PeftModel.from_pretrained(base_for_merge, "./log-analyzer-adapter")
merged = peft_model.merge_and_unload()
merged.save_pretrained("./log-analyzer_")
tokenizer.save_pretrained("./log-analyzer_")

#CHECKING OUTPUT before proceedinb to command prompt


# 1) Install bitsandbytes if you haven’t already (for 4-bit inference)
!pip install bitsandbytes

# 2) Imports
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# 3) Settings
MODEL_DIR = "./log-analyzer-merged"
DEVICE    = "cuda" if torch.cuda.is_available() else "cpu"

# 4) Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
tokenizer.pad_token_id = tokenizer.eos_token_id

# 5) Load the merged model in 4-bit
bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_DIR,
    quantization_config=bnb_cfg,
    device_map="auto"
)
model.eval()  # turn off dropout, etc.

# 6) Inference helper
def analyze_logs_chunk(log_chunk: str, max_new_tokens: int = 200) -> str:
    prompt = (
        "### Instruction:\n"
        "Analyze the following application logs to identify number of errors, order flow, and trace IDs for failures.\n\n"
        "### Input:\n"
        f"{log_chunk}\n\n"
        "### Response:\n"
    )
    # tokenize + move to GPU
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(DEVICE)
    # generate only new tokens
    out = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.1,
        top_p=0.9,
        repetition_penalty=1.1,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
        do_sample=True,
    )
    # slice off prompt, decode only new tokens
    gen = out[0][ inputs["input_ids"].size(1) : ]
    return tokenizer.decode(gen, skip_special_tokens=True).strip()

# 7) Load a small test chunk
with open("order_logs.jsonl") as f:
    logs = [json.loads(l) for l in f]
test_chunk = "\n".join(json.dumps(evt, indent=2) for evt in logs[:5])

# 8) Run and print
print("=== Sanity Check Output ===\n")
print(analyze_logs_chunk(test_chunk))

# Commented out IPython magic to ensure Python compatibility.
# convert this
# %cd /content
!git clone https://github.com/ggerganov/llama.cpp

# Commented out IPython magic to ensure Python compatibility.
# %cd llama.cpp

# install the requirements
!pip install -r requirements.txt

# convert final model to gguf format to deploy on ollama, openwebUI
!python3 convert_hf_to_gguf.py /content/log-analyzer_ --outfile /content/log-analyzer_.gguf

# save this gguf to your local environment to download quicker(optional)
from google.colab import drive
drive.mount('/content/drive')
!mkdir -p /content/drive/MyDrive/llm_models
!cp /content/log-analyzer-merged_v6.gguf /content/drive/MyDrive/llm_models/